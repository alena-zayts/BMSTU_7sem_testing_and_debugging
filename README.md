# BMSTU_7sem_testing_and_debugging
7th sem BMSTU, Testing and debugging

# Лабораторная 1 (SkiResort)
Написать unit-тесты для компонентов доступа к данным и бизнес логики курсовой работы по БД

1. Требуемое покрытие тестами: один класс - как минимум один test suite / test
class с как минимум с одним тестом на каждый из public-методов каждого класса
основных компонентов; если проект для тестирования выполнен не в
объектном стиле -- то необходимо выделить модули исходя из структуры
программы
2. Должны быть представлены тесты как в классическом (без mock \ stub) так и в
“Лондонском” варианте; допустимо представить один и тот же тест в обоих
вариантах для сравнения (в учебных целях, на практике это редко имеет смысл)
3. Должны быть представлены варианты использования mock \ stub
4. Должна быть соблюдена структура Arrange-Act-Assert для каждого теста c
использованием fixture и остальных классов\методов хелперов
5. Должны быть представлены тесты с использованием паттерна Data Builder и
Fabric(Object Mother) для генерации объектов для тестов
6. Должен быть настроен локальный запуск тестов в среде разработки
(опционально, если выполнен пункт 7)
7. Должен быть настроен запуск тестов из командной строки на основании
локальной копии репозитория (обязательно)
8. Должен быть представлен автоматически сгенерированный отчет по
результатам выполнения тестов (рекомендуется использовать allure
(https://github.com/allure-framework), кроме случаев, когда используемый язык
программирования не поддерживается); генерация отчета также должна быть
учтена в пункта 7 и 6
9. Защита от регрессии, устойчивость к рефакторингу и легкость поддержки --
базовые принципы, которым стоит следовать


# Лабораторная 2 (SkiResort)

Задание:
1. Написать integration-тесты для компонентов доступа к данным и бизнес логики
курсовой работы по БД
2. Написать E2E-тест для демонстрационного сценария (например того, который
был использован при защите курсовой работы)
3. Организовать запуск тестов из лабораторной работы №1 и вновь добавленных
в рамках работы №2 в любой, на усмотрение студента, CI\CD среде (GitLab,
Jenkins, TeamCity, etc.)
4. Проимитировать действия из Е2Е теста с помощью средства для отправки
запросов, снять лог с помощью средства захвата траффика


Требования
1. Интеграционные тесты должны включать в себя взаимодействие с БД
2. Для тестов должен инициализироваться отдельный инстанс БД
3. Инстанс БД поднимать с помощью последовательного запуска скриптов –
генерация таблиц, заведение тестовых данных, откат состояния БД на то,
которое было до прогона тестов
4. Порядок запуска тестов в CI\CD: unit → integration → e2e
5. Если один из этапов свалился, последующие запускать не нужно
6. Если свалился integration или e2e этап, то требуется принудительно откатить БД
на состояние до запуска тестов (но не на состояние, когда таблицы ещё не
созданы)
7. Если используется service bus / message broker / шина / и т.д. и т.п. то после
аварийного или успешного завершения тестов требуется произвести
вычитывание всех сообщений из очереди
8. Если в проекте используется хранение активных сессией пользователей в том
или ином виде, то требуется предусмотреть завершение всех сессий после
аварийного или успешного завершения тестов
9. Не требуется включать этап тестирования GUI в E2E тест
10. Убедиться, что тесты могут быть запущены несколькими разработчиками
локально одновременно без влияния на результаты друг друга
11. Рекомендации по структуре файлов с тестами:
a. один файл – один тест из нескольких шагов (секций act, assert)
b. arrange по возможности желательно сделать один раз в начале теста;
c. если для unit-тестов рекомендуется использовать суффикс
test\Test\_test\test_\и т.д. в зависимости от языка программирования, то
для интеграционных также лучше использовать соответствующий
суффикс, например для Java это ITCase (сокращение от Integration Test
Сase) – уточняйте для выбранного языка программирования сами
12. Желательно оформить запуск тестов в виде отдельного контейнера docker:
выкачивается репозиторий с кодом, устанавливаются все внешние зависимости
и всё для запуск тестов на используемом языке программирования, контейнер
вызывается внутри выбранной CI\CD среды


# Лабораторная 3 (MyBenchmark)

Задание:

Составить набор сценариев (benchmark) для оценки производительности фреймворка
для проекта
Примеры часто рассматриваемых сценариев для оценки производительности можно
найти например в сравнении разных популярных web-фреймворков:
- https://github.com/the-benchmarker/website
- https://github.com/TechEmpower/FrameworkBenchmarks
Так же требуется сравнить хотя бы с одной альтернативой выбранному фреймворку


Требования:
1. Т.к. производительность (не важно – по памяти на сценарий, по времени
обработки запроса на сервере, по задержке из-за сериализации объектов, по
количеству полных gc-пауз если это jvm, по времени warm-up периода при
старте приложения) измеряется статистической величиной, то требуется
провести хотя бы 100 испытаний и собрать статистику
2. Т.к. испытаний будет много, желательно добиться равных условий на каждую
попытку, то есть следует использовать отдельный докер-образ на каждый
прогон, но с одной и той же конфигурацией
3. Следует проверить хотя бы 2-3 параметра: если benchmark для
web-фреймворка, то например сериализация объектов + время обработки
тяжелых и средних запросов на бэкенде + одновременный логин большого (но
допустимого) количества пользователей; если подразумевается непрерывность
работы или выбор “самого доступного на данный момент” узла – то время
переключения на новый узел в случае загруженности ранее использованного,
фактически время ожидания заявки / пользователя / запроса в очереди на
ожидание
4. Ожидается такой способ использования benchmark:
a. поднимается докер образ с выбранным объектом оценки
b. запускается набор тестов для оценки важных параметров
c. собирается статистика с такого прогона (на основании лога, либо через
условный attach каких-то артефактов к шагам сценария и их анализом,
ключевое – данные должны быть обработаны автоматически либо
сгруппированы в .csv, на который можно натравить тот же excel)
d. выполняем эти шаги 100 раз
e. собирается окончательная статистика по всем прогонам и формируется
отчет
f. проверяется альтернативный объект тем же способом и формируется
отчет
